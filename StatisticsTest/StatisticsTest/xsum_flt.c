// Floating-point numbers summation.
//
// Implementation of algorithms for summing an array of floating-point numbers. The main
// goal is to study the differences among these methods.
//
// See the following references for background information:
//
// [1] Borgwardt, M. (n.d.). What every programmer should know about floating-point
//     arithmetic. Retrieved December 1, 2019, from https://floating-point-gui.de/
//
// [2] Cook, J. D. (2019, November 5). Summing an array of floating point numbers.
//     Retrieved December 1, 2019, from https://www.johndcook.com/blog/2019/11/05/kahan/
//
// [3] McNamee, J. M. (2004). A comparison of methods for accurate summation. ACM SIGSAM
//     Bulletin, 38(1), 1-7. https://doi.org/10.1145/980175.980177
//
// Copyright (c) 2019 Jorge Ramos (mailto jramos at pobox dot com)
//
// This is free software. Redistribution and use in source and binary forms,
// with or without modification, for any purpose and with or without fee are
// hereby permitted. Altered source versions must be plainly marked as such.
//
// If you find this software useful, an acknowledgment would be appreciated
// but is not required.
//
// THIS SOFTWARE IS PROVIDED "AS IS", WITHOUT ANY WARRANTY OR CONDITION.
// IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE TO ANYONE
// FOR ANY DAMAGES RELATED TO THIS SOFTWARE, UNDER ANY KIND OF LEGAL CLAIM.

#include "bugcheck.h"
#include "xsum_flt.h"

// Finds the log2 of an integer. Returns -1 on non-positive input values.
//
// This function is based on the IntegerLogObvious algorithm proposed by Anderson (2005)
// as follows:
//
//     unsigned int v; // 32-bit word to find the log base 2 of
//     unsigned int r = 0; // r will be lg(v)
//     
//     while (v >>= 1) // unroll for more speed...
//     {
//       r++;
//     }
//     
//     The log base 2 of an integer is the same as the position of the highest bit set (or
//     most significant bit set, MSB).
//
// [1] Anderson, S. E. (2005). Find the log base 2 of an integer with the MSB N set in O(N)
//     operations (the obvious way). Bit Twiddling Hacks. Retrieved November 24, 2019, from
//     https://graphics.stanford.edu/~seander/bithacks.html#IntegerLogObvious

static int ilog2(int value)
{
    if (value < 1)
    {
        return -1;
    }

    int result = 0;

    while (value >>= 1)
    {
        ++result;
    }

    return result;
}

// Returns a random integer that is within a specified range.
//
// This function uses a uniform deviate, as suggested by Press (1992) and Walker (2005),
// to convert a random number generated by rand() to the range [0..1). The result is then
// scaled to the desired range using the formula proposed by Walker (2005).
//
// [1] Press, W. H. et al. (1992). Sample pages [PDF file]. In Numerical Recipes in C: The
//     Art of Scientific Computing (2nd ed., pp. 275-277). Retrieved from
//     http://www.foo.be/docs-free/Numerical_Recipe_In_C/c7-1.pdf
//
// [2] Walker, J. (2005). Using rand(). Eternally Confuzzled. Retrieved from
//     https://web.archive.org/web/20060902124145/http://eternallyconfuzzled.com/articles/rand.html

static int xrand(int minval, int maxval)
{
    if (maxval < minval)
    {
        int temp = minval;
        minval = maxval;
        maxval = temp;
    }

    float range = (float)maxval - minval;
    float value = rand() * (1.0f / (RAND_MAX + 1.0f));
    return minval + (int)(range * value);
}

// Comparison function to sort an array of floats in ascending order of absolute value.

static int bsf_flt_cmp_absa(const void* pl, const void* pr)
{
    float l = fabsf(*(const float*)pl);
    float r = fabsf(*(const float*)pr);

    return l < r ? -1 : l > r ? 1 : 0;
}

// Comparison function to sort an array of floats in ascending order of absolute value.
// This comparison function breaks absolute value ties to attain complete predictability.

static int bsf_flt_cmp_absa2(const void* pl, const void* pr)
{
    float vl = *(const float*)pl;
    float vr = *(const float*)pr;
    float fl = fabsf(vl);
    float fr = fabsf(vr);

    return fl < fr ? -1 : fl > fr ? 1 : vl < vr ? -1 : vl > vr ? 1 : 0;
}

// Comparison function to sort an array of floats in descending order of absolute value.

static int bsf_flt_cmp_absd(const void* pl, const void* pr)
{
    float l = fabsf(*(const float*)pl);
    float r = fabsf(*(const float*)pr);

    return l < r ? 1 : l > r ? -1 : 0;
}

// Comparison function to sort an array of floats in descending order of absolute value.
// This comparison function breaks absolute value ties to attain complete predictability.

static int bsf_flt_cmp_absd2(const void* pl, const void* pr)
{
    float vl = *(const float*)pl;
    float vr = *(const float*)pr;
    float fl = fabsf(vl);
    float fr = fabsf(vr);

    return fl < fr ? 1 : fl > fr ? -1 : vl < vr ? 1 : vl > vr ? -1 : 0;
}

// Straightforward summation algorithm.
//
// Beware of rounding errors.

static double bsf_dbl_sum(const float* pdata, int start, int length)
{
    double s = DBL_ZERO;

    for (pdata += start; length > 0; --length, ++pdata)
    {
        s += *pdata;
    }

    return s;
}

// Straightforward summation algorithm.
//
// Beware of rounding errors.

float bsf_flt_sum(const float* pdata, int start, int length)
{
    float s = FLT_ZERO;

    for (pdata += start; length > 0; --length, ++pdata)
    {
        s += *pdata;
    }

    return s;
}

// Pairwise summation algorithm.
//
// From Wikipedia (2019):
//
//     In numerical analysis, pairwise summation, also called cascade summation, is a
//     technique to sum a sequence of finite-precision floating-point numbers that
//     substantially reduces the accumulated round-off error compared to naively
//     accumulating the sum in sequence.
//
//     Although there are other techniques such as Kahan summation that typically have
//     even smaller round-off errors, pairwise summation is nearly as good (differing only
//     by a logarithmic factor) while having much lower computational cost. It can be
//     implemented so as to have nearly the same cost (and exactly the same number of
//     arithmetic operations) as naive summation.
//
// This function is based on the Formula 6 algorithm proposed by Linz (1970, p. 361) as
// follows:
//
//     S[i][0] = a[i],                              i = 1, 2, ..., n,
//     S[i][j] = S[2*i][j-1] + S[2*i-1][j-1],       i = 1, 2, ..., n / (2 ** j),
//                                                  j = 1, 2, ..., k,
//     S = S[i][k].
//     
//     For simplicity we assume that n is an integral power of 2 (n = 2 ** k). This scheme
//     consists of first adding a[l] to a[2], a[3] to a[4], ..., then, at the next step,
//     (a[l] + a[2]) to (a[3] + a[4]), (a[5] + a[6]) to (a[7] + a[8]), ..., and so on.
//
// See also Manning (1996) for another implementation of the same algorithm.
//
// The algorithm described in Wikipedia (2019) is slightly different from Linz's (1970)
// work. Both perform the same operations, but these operations are performed in the same
// sequence only when the number of summands is a power of two. Other than that, they are
// similar.
//
// [1] Linz, P. (1970). Accurate floating-point summation. Communications of the ACM,
//     13(6), 361–362. https://doi.org/10.1145/362384.362498
//
// [2] Manning, E. (1996). Floating-point summation. Dr. Dobb's Journal. Retrieved from
//     https://www.drdobbs.com/floating-point-summation/184403224
//
// [3] Wikipedia contributors. (2019, June 1). Pairwise summation.
//     In Wikipedia, The Free Encyclopedia. Retrieved November 24, 2019, from
//     https://en.wikipedia.org/w/index.php?title=Pairwise_summation&oldid=899870482

float bsf_flt_sum_pairwise_linz(const float* pdata, int start, int length)
{
    int k = (length + 1) / 2;
    float* m = (float*)malloc(k * sizeof(float));
    int n = length / 2;
    int i = 0;
    int j = 0;

    while (i < n)
    {
        float v1 = pdata[start + j++];
        float v2 = pdata[start + j++];
        m[i++] = v1 + v2;
    }

    if (k > n)
    {
        m[i++] = pdata[start + j++];
    }

    bugcheck(i == k);
    bugcheck(j == length);

    while (k > 1)
    {
        length = k;
        k = (length + 1) / 2;
        n = length / 2;
        i = 0;
        j = 0;

        while (i < n)
        {
            float v1 = m[j++];
            float v2 = m[j++];
            m[i++] = v1 + v2;
        }

        if (k > n)
        {
            m[i++] = m[j++];
        }

        bugcheck(i == k);
        bugcheck(j == length);
    }

    float s = m[0];
    free(m);
    return s;
}

// Pairwise summation algorithm (Wikipedia variation).
//
// From Wikipedia (2019):
//
//     In numerical analysis, pairwise summation, also called cascade summation, is a
//     technique to sum a sequence of finite-precision floating-point numbers that
//     substantially reduces the accumulated round-off error compared to naively
//     accumulating the sum in sequence.
//
//     Although there are other techniques such as Kahan summation that typically have
//     even smaller round-off errors, pairwise summation is nearly as good (differing only
//     by a logarithmic factor) while having much lower computational cost. It can be
//     implemented so as to have nearly the same cost (and exactly the same number of
//     arithmetic operations) as naive summation.
//
// This function is based on the algorithm described in Wikipedia (2019), which is
// slightly different from Linz's (1970) work. Both perform the same operations, but
// these operations are performed in the same sequence only when the number of summands
// is a power of two. Other than that, they are similar.
//
// The pairwise summation algorithm as described in Wikipedia (2019):
//
//     s = pairwise(x[1...n])
//         if n <= N         base case: naive summation for a sufficiently small array
//           s = x[1]
//           for i = 2 to n
//             s = s + x[i]
//         else              divide and conquer: recursively sum two halves of the array
//           m = floor(n / 2)
//           s = pairwise(x[1...m]) + pairwise(x[m+1...n])
//         endif
//
// [1] Linz, P. (1970). Accurate floating-point summation. Communications of the ACM,
//     13(6), 361–362. https://doi.org/10.1145/362384.362498
//
// [2] Wikipedia contributors. (2019, June 1). Pairwise summation.
//     In Wikipedia, The Free Encyclopedia. Retrieved November 24, 2019, from
//     https://en.wikipedia.org/w/index.php?title=Pairwise_summation&oldid=899870482

float bsf_flt_sum_pairwise_wikipedia(const float* pdata, int start, int length)
{
    switch (length)
    {
    case 0:
        return FLT_ZERO;
    case 1:
        return pdata[start];
    case 2:
        return pdata[start] + pdata[start + 1];
    case 3:
        return pdata[start] + pdata[start + 1] + pdata[start + 2];
    case 4:
        return (pdata[start] + pdata[start + 1]) + (pdata[start + 2] + pdata[start + 3]);
    }

    int m = length / 2;
    float s1 = bsf_flt_sum_pairwise_wikipedia(pdata, start, m);
    float s2 = bsf_flt_sum_pairwise_wikipedia(pdata, start + m, length - m);
    return s1 + s2;
}

// Pairwise summation algorithm (Wikipedia variation updated).
//
// From Wikipedia (2019):
//
//     In numerical analysis, pairwise summation, also called cascade summation, is a
//     technique to sum a sequence of finite-precision floating-point numbers that
//     substantially reduces the accumulated round-off error compared to naively
//     accumulating the sum in sequence.
//
//     Although there are other techniques such as Kahan summation that typically have
//     even smaller round-off errors, pairwise summation is nearly as good (differing only
//     by a logarithmic factor) while having much lower computational cost. It can be
//     implemented so as to have nearly the same cost (and exactly the same number of
//     arithmetic operations) as naive summation.
//
// The algorithm described in Wikipedia (2019) is slightly different from Linz's (1970)
// work. Both perform the same operations, but these operations are performed in the same
// sequence only when the number of summands is a power of two. Other than that, they are
// similar.
//
// The pairwise summation algorithm as described in Wikipedia (2019):
//
//     s = pairwise(x[1...n])
//         if n <= N         base case: naive summation for a sufficiently small array
//           s = x[1]
//           for i = 2 to n
//             s = s + x[i]
//         else              divide and conquer: recursively sum two halves of the array
//           m = floor(n / 2)
//           s = pairwise(x[1...m]) + pairwise(x[m+1...n])
//         endif
//
// The Wikipedia (2019) algorithm breaks the sequence at half. This function breaks the
// sequence at powers of two, replacing "floor(n / 2)" by "floor(2 ** log2(n - 1)) in the
// algorithm. The result is the same when "n" is a power of two. When "n" is not a power
// of two, at least one of the new sequences will have a length that is a power of two.
// This method tries to approximate to the result calculated by the original Litz's (1970)
// algorithm.
//
// The pairwise summation algorithm as implemented here:
//
//     s = pairwise(x[1...n])
//         if n <= N         base case: naive summation for a sufficiently small array
//           s = x[1]
//           for i = 2 to n
//             s = s + x[i]
//         else              divide and conquer: recursively sum two halves of the array
//           m = floor(2 ** log2(n - 1))
//           s = pairwise(x[1...m]) + pairwise(x[m+1...n])
//         endif
//
// [1] Linz, P. (1970). Accurate floating-point summation. Communications of the ACM,
//     13(6), 361–362. https://doi.org/10.1145/362384.362498
//
// [2] Wikipedia contributors. (2019, June 1). Pairwise summation.
//     In Wikipedia, The Free Encyclopedia. Retrieved November 24, 2019, from
//     https://en.wikipedia.org/w/index.php?title=Pairwise_summation&oldid=899870482

float bsf_flt_sum_pairwise_wikipedia2(const float* pdata, int start, int length)
{
    switch (length)
    {
    case 0:
        return FLT_ZERO;
    case 1:
        return pdata[start];
    case 2:
        return pdata[start] + pdata[start + 1];
    case 3:
        return pdata[start] + pdata[start + 1] + pdata[start + 2];
    case 4:
        return (pdata[start] + pdata[start + 1]) + (pdata[start + 2] + pdata[start + 3]);
    }

    int m = 1 << ilog2(length - 1);
    float s1 = bsf_flt_sum_pairwise_wikipedia2(pdata, start, m);
    float s2 = bsf_flt_sum_pairwise_wikipedia2(pdata, start + m, length - m);
    return s1 + s2;
}

// Kahan summation algorithm.
//
// From Wikipedia (2019):
//
//     In numerical analysis, the Kahan summation algorithm, also known as compensated
//     summation, significantly reduces the numerical error in the total obtained by
//     adding a sequence of finite-precision floating-point numbers, compared to the 
//     obvious approach. This is done by keeping a separate running compensation (a
//     variable to accumulate small errors).
//
//     In particular, simply summing n numbers in sequence has a worst-case error that
//     grows proportional to n, and a root mean square error that grows as sqrt(n) for 
//     random inputs (the roundoff errors form a random walk). With compensated summation, 
//     the worst-case error bound is independent of n, so a large number of values can be 
//     summed with an error that only depends on the floating-point precision.
//
// This function is based on the original version of Kahan's (1965) summation code in
// FORTRAN as follows:
//
//     S = 0.0
//     S2 = 0.0
//     DO 4 I = 1, N
//     YI = ...
//     S2 = S2 + YI
//     T = S + S2
//     S2 = (S - T) + S2
//   4 S = T
//
// The algorithm described in Wikipedia (2019) is slightly different from Kahan's (1965)
// work. Both perform the same operations, but the algorithm in Wikipedia calculates a
// positive compensation equivalent to -S2 at each step and then subtracts it from the
// next input value. Other than that, they are similar.
//
// [1] Kahan, W. (1965). Further remarks on reducing truncation errors. Communications of
//     the ACM, 8(1), 40. https://doi.org/10.1145/363707.363723
//
// [2] Wikipedia contributors. (2019, October 30). Kahan summation algorithm.
//     In Wikipedia, The Free Encyclopedia. Retrieved November 25, 2019, from
//     https://en.wikipedia.org/w/index.php?title=Kahan_summation_algorithm&oldid=923781580

float bsf_flt_sum_kahan(const float* pdata, int start, int length)
{
    float s = FLT_ZERO;
    float s2 = FLT_ZERO;

    for (pdata += start; length > 0; --length, ++pdata)
    {
        float yi = *pdata;
        s2 = s2 + yi;
        float t = s + s2;
        s2 = (s - t) + s2;
        s = t;
    }

    return s;
}

// Kahan summation algorithm (Muller variation).
//
// From Wikipedia (2019):
//
//     In numerical analysis, the Kahan summation algorithm, also known as compensated
//     summation, significantly reduces the numerical error in the total obtained by
//     adding a sequence of finite-precision floating-point numbers, compared to the 
//     obvious approach. This is done by keeping a separate running compensation (a
//     variable to accumulate small errors).
//
//     In particular, simply summing n numbers in sequence has a worst-case error that
//     grows proportional to n, and a root mean square error that grows as sqrt(n) for 
//     random inputs (the roundoff errors form a random walk). With compensated summation, 
//     the worst-case error bound is independent of n, so a large number of values can be 
//     summed with an error that only depends on the floating-point precision.
//
// This function is based on the Algorithm 5.6 in Muller et al. (2018), which is similar
// to the algorithm in Wikipedia (2019), but is slightly different from Kahan's (1965)
// work. The Algorithm 5.6 calculates a positive compensation and subtracts it from the 
// next input value. Kahan uses a negative compensation and adds it to the next input
// value. Other than that, they are similar.
//
// The Algorithm 5.6 (Muller et al., 2018, p. 179):
//
//     s := x[1]
//     c := 0
//     for i = 2 to n do
//       y := fl(x[i] - c)
//       t := fl(s + y)
//       c := fl(fl(t - s) - y)
//       s := t
//     end for
//     return s
//
// fl(E) is the expression obtained by replacing the arithmetical operations "+", "-", "*"
// and "/" present in E by the corresponding floating-point operations.
//
// [1] Kahan, W. (1965). Further remarks on reducing truncation errors. Communications of
//     the ACM, 8(1), 40. https://doi.org/10.1145/363707.363723
//
// [2] Muller, JM. et al. (2018). Enhanced floating-point sums, dot products, and
//     polynomial values. In Handbook of Floating-Point Arithmetic (pp. 163–192).
//     Birkhäuser. https://doi.org/10.1007/978-3-319-76526-6_5
//
// [2] Wikipedia contributors. (2019, October 30). Kahan summation algorithm.
//     In Wikipedia, The Free Encyclopedia. Retrieved November 25, 2019, from
//     https://en.wikipedia.org/w/index.php?title=Kahan_summation_algorithm&oldid=923781580

float bsf_flt_sum_kahan_muller(const float* pdata, int start, int length)
{
    float s = FLT_ZERO;
    float c = FLT_ZERO;

    for (pdata += start; length > 0; --length, ++pdata)
    {
        float y = *pdata - c;
        float t = s + y;
        c = (t - s) - y;
        s = t;
    }

    return s;
}

// Kahan summation algorithm (Muller variation with Fast2Sum).
//
// From Wikipedia (2019):
//
//     In numerical analysis, the Kahan summation algorithm, also known as compensated
//     summation, significantly reduces the numerical error in the total obtained by
//     adding a sequence of finite-precision floating-point numbers, compared to the 
//     obvious approach. This is done by keeping a separate running compensation (a
//     variable to accumulate small errors).
//
//     In particular, simply summing n numbers in sequence has a worst-case error that
//     grows proportional to n, and a root mean square error that grows as sqrt(n) for 
//     random inputs (the roundoff errors form a random walk). With compensated summation, 
//     the worst-case error bound is independent of n, so a large number of values can be 
//     summed with an error that only depends on the floating-point precision.
//
// This function is based on the Algorithm 5.7 in Muller et al. (2018), which is similar
// to the Kahan's (1965) summation algorithm, rewritten to use Fast2Sum.
//
// The Algorithm 5.7 (Muller et al., 2018, p. 179):
//
//     s := x[1]
//     c := 0
//     for i = 2 to n do
//       y := fl(x[i] + c)
//       (s, c) := Fast2Sum(s, y)
//     end for
//     return s
//
// fl(E) is the expression obtained by replacing the arithmetical operations "+", "-", "*"
// and "/" present in E by the corresponding floating-point operations.
//
// The algorithm described in Wikipedia (2019) is slightly different from Algorithm 5.7
// (Muller et al., 2018). The Wikipedia vesion calculates a positive compensation and
// subtracts it from the next input value, while Algorithm 5.7, as in Kahan (1965), uses
// a negative compensation and adds it to the next input value. Other than that, they are
// similar.
//
// [1] Kahan, W. (1965). Further remarks on reducing truncation errors. Communications of
//     the ACM, 8(1), 40. https://doi.org/10.1145/363707.363723
//
// [2] Muller, JM. et al. (2018). Enhanced floating-point sums, dot products, and
//     polynomial values. In Handbook of Floating-Point Arithmetic (pp. 163–192).
//     Birkhäuser. https://doi.org/10.1007/978-3-319-76526-6_5
//
// [2] Wikipedia contributors. (2019, October 30). Kahan summation algorithm.
//     In Wikipedia, The Free Encyclopedia. Retrieved November 25, 2019, from
//     https://en.wikipedia.org/w/index.php?title=Kahan_summation_algorithm&oldid=923781580

float bsf_flt_sum_kahan_muller_fast2sum(const float* pdata, int start, int length)
{
    float s = FLT_ZERO;
    float c = FLT_ZERO;
    float t;

    for (pdata += start; length > 0; --length, ++pdata)
    {
        float y = *pdata + c;
        Fast2Sum(s, y, t, c);
        s = t;
    }

    return s;
}

// Priest simply compensated summation algorithm.
//
// This function is based on the simply compensated summation algorithm proposed by
// Priest (1992). The algorithm assumes the summands are ordered by decreasing magnitudes.
//
// The simply compensated summation algorithm (Priest, 1992, pp. 59-60):
//
//     procedure comp_sum(n; x[1]...x[n])
//     begin
//       s[1] := x[1], c[1] := 0
//       for k = 2 to n
//         y[k] := fl(c[k-1] + x[k])
//         s[k] := fl(y[k] + s[k-1])
//         u[k] := fl(s[k] - s[k-1])
//         c[k] := fl(y[k] - u[k])
//       return s[n]
//     end
//
// fl(E) is the expression obtained by replacing the arithmetical operations "+", "-", "*"
// and "/" present in E by the corresponding floating-point operations.
//
// [1] Priest, D. M. (1992). On properties of floating point arithmetics: numerical
//     stability and the cost of accurate computations (Doctoral Dissertation, University
//     of California at Berkeley). Retrieved from
//     https://pdfs.semanticscholar.org/4ba0/df0935641e440c0853a55d1410bbe0cd459b.pdf

float bsf_flt_sum_priest_comp(const float* pdata, int start, int length)
{
    float s = FLT_ZERO;
    float c = FLT_ZERO;

    for (pdata += start; length > 0; --length, ++pdata)
    {
        float x = *pdata;
        float y = c + x;
        float t = y + s;
        float u = t - s;
        c = y - u;
        s = t;
    }

    return s;
}

// Priest doubly compensated summation algorithm.
//
// This function is based on the doubly compensated summation algorithm proposed by
// Priest (1992). The algorithm assumes the summands are ordered by decreasing magnitudes.
//
// The doubly compensated summation algorithm (Priest, 1992, p. 64):
//
//     procedure dcomp_sum(n; x[1]...x[n])
//     begin
//       s[1] := x[1], c[1] := 0
//       for k = 2 to n
//         y[k] := fl(c[k-1] + x[k])
//         u[k] := fl(x[k] - fl(y[k] - c[k-1]))
//         t[k] := fl(y[k] + s[k-1])
//         v[k] := fl(y[k] - fl(t[k] - s[k-1]))
//         z[k] := fl(u[k] + v[k])
//         s[k] := fl(t[k] + z[k])
//         c[k] := fl(z[k] - fl(s[k] - t[k]))
//       return s[n]
//     end
//
// fl(E) is the expression obtained by replacing the arithmetical operations "+", "-", "*"
// and "/" present in E by the corresponding floating-point operations.
//
// [1] Priest, D. M. (1992). On properties of floating point arithmetics: numerical
//     stability and the cost of accurate computations (Doctoral Dissertation, University
//     of California at Berkeley). Retrieved from
//     https://pdfs.semanticscholar.org/4ba0/df0935641e440c0853a55d1410bbe0cd459b.pdf

float bsf_flt_sum_priest_dcomp(const float* pdata, int start, int length)
{
    float s = FLT_ZERO;
    float c = FLT_ZERO;

    for (pdata += start; length > 0; --length, ++pdata)
    {
        float x = *pdata;
        float y = c + x;
        float u = x - (y - c);
        float t = y + s;
        float v = y - (t - s);
        float z = u + v;
        s = t + z;
        c = z - (s - t);
    }

    return s;
}

// Priest doubly compensated summation algorithm rewritten with Fast2Sum.
//
// This function is based on the Algorithm 5.9 in Muller et al. (2018), which in turn is a
// variation of the Priest's (1992) doubly compensated summation algorithm, rewritten to
// use Fast2Sum. Algorithm 5.9 assumes the summands are ordered by decreasing magnitudes.
//
// The Algorithm 5.9 (Muller et al., 2018, p. 181):
//
//     s[1] := x[1]
//     c[1] := 0
//     for i = 2 to n do
//       (y[i], u[i]) := Fast2Sum(c[i-1], x[i])
//       (t[i], v[i]) := Fast2Sum(s[i-1], y[i])
//       z[i] := fl(u[i] + v[i])
//       (s[i], c[i]) := Fast2Sum(t[i], z[i])
//     end for
//
// fl(E) is the expression obtained by replacing the arithmetical operations "+", "-", "*"
// and "/" present in E by the corresponding floating-point operations.
//
// [1] Muller, JM. et al. (2018). Enhanced floating-point sums, dot products, and
//     polynomial values. In Handbook of Floating-Point Arithmetic (pp. 163–192).
//     Birkhäuser. https://doi.org/10.1007/978-3-319-76526-6_5
//
// [2] Priest, D. M. (1992). On properties of floating point arithmetics: numerical
//     stability and the cost of accurate computations (Doctoral Dissertation, University
//     of California at Berkeley). Retrieved from
//     https://pdfs.semanticscholar.org/4ba0/df0935641e440c0853a55d1410bbe0cd459b.pdf

float bsf_flt_sum_priest_dcomp_fast2sum(const float* pdata, int start, int length)
{
    float s = FLT_ZERO;
    float c = FLT_ZERO;
    float y;
    float u;
    float t;
    float v;

    for (pdata += start; length > 0; --length, ++pdata)
    {
        float x = *pdata;
        Fast2Sum(c, x, y, u);
        Fast2Sum(s, y, t, v);
        float z = u + v;
        Fast2Sum(t, z, s, c);
    }

    return s;
}

// Neumaier summation algorithm.
//
// From Wikipedia (2019):
//
//     Neumaier introduced an improved version of Kahan algorithm, which he calls an
//     "improved Kahan–Babuška algorithm", which also covers the case when the next term
//     to be added is larger in absolute value than the running sum, effectively swapping
//     the role of what is large and what is small.
//
// The NeumaierSum algorithm (Wikipedia, 2019):
//
//     function NeumaierSum(input)
//     var sum = 0.0
//     var c = 0.0                    // A running compensation for lost low-order bits.
//     for i = 1 to input.length do
//       var t = sum + input[i]
//       if |sum| >= |input[i]| then
//         c += (sum - t) + input[i]  // If sum is bigger, low-order digits of input[i]
//       else                         //  are lost.
//         c += (input[i] - t) + sum  // Else low-order digits of sum are lost.
//       endif
//       sum = t
//     next i
//     return sum + c                 // Correction only applied once in the very end.
//
// [1] Wikipedia contributors. (2019, October 30). Kahan summation algorithm.
//     In Wikipedia, The Free Encyclopedia. Retrieved November 25, 2019, from
//     https://en.wikipedia.org/w/index.php?title=Kahan_summation_algorithm&oldid=923781580

float bsf_flt_sum_neumaier(const float* pdata, int start, int length)
{
    float s = FLT_ZERO;
    float c = FLT_ZERO;

    for (pdata += start; length > 0; --length, ++pdata)
    {
        float y = *pdata;
        float t = s + y;
        c += fabsf(s) >= fabsf(y) ? (s - t) + y : (y - t) + s;
        s = t;
    }

    return s + c;
}

// Neumaier summation algorithm (Muller variation with Fast2Sum)
//
// From Muller et al. (2018):
//
//     In Kahan’s algorithm (Algorithm 5.7), in many practical cases c will have much
//     smaller magnitude than xi, so that when adding them together, a large part of the
//     information contained in variable c may be lost. Indeed, Priest’s algorithm also
//     compensates for the error of this addition, hence the name "doubly compensated
//     summation."
//
//     To deal with that problem, Pichat [490] and Neumaier [454] independently found the
//     same idea: at step i, the rounding error,10 say ei, is still computed due to the
//     addition of xi. However, instead of immediately subtracting ei from the next
//     operand, the terms ek are added together, to get a correcting term e that will be
//     added to s at the end of the computation. See Algorithm 5.10.
//
// The Algorithm 5.10 (Muller et al., 2018, p. 182):
//
//     s := x[1]
//     e := 0
//     for i := 2 to n do
//       if |s| >= |x[i]| then
//         (s, e[i]) := Fast2Sum(s, x[i])
//       else
//         (s, e[i]) := Fast2Sum(x[i], s)
//       end if
//       e := RN(e + e[i])
//     end for
//     return RN(s + e)
//
// RN(x) is the round to nearest function.
//
// [1] Muller, JM. et al. (2018). Enhanced floating-point sums, dot products, and
//     polynomial values. In Handbook of Floating-Point Arithmetic (pp. 163–192).
//     Birkhäuser. https://doi.org/10.1007/978-3-319-76526-6_5
//
// [2] Neumaier, A. (1974). Rundungsfehleranalyse einiger verfahren zur summation
//     endlicher summen [Rounding error analysis of some methods for summing finite sums].
//     Zeitschrift für Angewandte Mathematik und Mechanik, 54(1), 39–51.
//     https://doi.org/10.1002/zamm.19740540106
//
// [3] Pichat, M. (1972). Correction d'une somme en arithmetique a virgule flottante
//     [Correction of a sum in floating-point arithmetic]. Numerische Mathematik, 19(5),
//     400–406. https://doi.org/10.1007/BF01404922
//

float bsf_flt_sum_neumaier_fast2sum(const float* pdata, int start, int length)
{
    float s = FLT_ZERO;
    float e = FLT_ZERO;
    float t;
    float c;

    for (pdata += start; length > 0; --length, ++pdata)
    {
        float y = *pdata;

        if (fabsf(s) >= fabsf(y))
            Fast2Sum(s, y, t, c);
        else
            Fast2Sum(y, s, t, c);

        e += c;
        s = t;
    }

    return s + e;
}

// Neumaier summation algorithm (Muller variation with TwoSum)
//
// From Muller et al. (2018):
//
//     In Kahan’s algorithm (Algorithm 5.7), in many practical cases c will have much
//     smaller magnitude than xi, so that when adding them together, a large part of the
//     information contained in variable c may be lost. Indeed, Priest’s algorithm also
//     compensates for the error of this addition, hence the name "doubly compensated
//     summation."
//
//     To deal with that problem, Pichat [490] and Neumaier [454] independently found the
//     same idea: at step i, the rounding error,10 say ei, is still computed due to the
//     addition of xi. However, instead of immediately subtracting ei from the next
//     operand, the terms ek are added together, to get a correcting term e that will be
//     added to s at the end of the computation. See Algorithm 5.10.
//
//     To avoid tests, the algorithm of Pichat and Neumaier can be rewritten using the
//     2Sum algorithm (it also has the advantage of working in any radix). This gives the
//     cascaded summation algorithm of Rump, Ogita, and Oishi [471]. It always gives
//     exactly the same result as Pichat and Neumaier’s algorithm, but it does not need
//     comparisons. See Algorithm 5.11.
//
// The Algorithm 5.11 (Muller et al., 2018, p. 182):
//
//     s := x[1]
//     e := 0
//     for i := 2 to n do
//       (s, e[i]) := 2Sum(s, x[i])
//       e := RN(e + e[i])
//     end for
//     return RN(s + e)
//
// RN(x) is the round to nearest function.
//
// [1] Muller, JM. et al. (2018). Enhanced floating-point sums, dot products, and
//     polynomial values. In Handbook of Floating-Point Arithmetic (pp. 163–192).
//     Birkhäuser. https://doi.org/10.1007/978-3-319-76526-6_5
//
// [2] Neumaier, A. (1974). Rundungsfehleranalyse einiger verfahren zur summation
//     endlicher summen [Rounding error analysis of some methods for summing finite sums].
//     Zeitschrift für Angewandte Mathematik und Mechanik, 54(1), 39–51.
//     https://doi.org/10.1002/zamm.19740540106
//
// [3] Pichat, M. (1972). Correction d'une somme en arithmetique a virgule flottante
//     [Correction of a sum in floating-point arithmetic]. Numerische Mathematik, 19(5),
//     400–406. https://doi.org/10.1007/BF01404922
//
// [4] Rump, S. M., Ogita, T., & Oishi, S. (2008). Accurate floating-point summation
//     Part I: Faithful rounding. SIAM Journal on Scientific Computing, 31(1), 189–224.
//     https://doi.org/10.1137/050645671

float bsf_flt_sum_neumaier_twosum(const float* pdata, int start, int length)
{
    float s = FLT_ZERO;
    float e = FLT_ZERO;
    float t;
    float c;

    for (pdata += start; length > 0; --length, ++pdata)
    {
        float y = *pdata;
        TwoSum(s, y, t, c);
        e += c;
        s = t;
    }

    return s + e;
}

float bsf_flt_sum_neumaier_twosum2(const float* pdata, int start, int length)
{
    float s = FLT_ZERO;
    float c = FLT_ZERO;

    for (pdata += start; length > 0; --length, ++pdata)
    {
        float y = *pdata;
        float t = s + y;
        float w = t - s;
        c += (y - w) - ((t - w) - s);
        s = t;
    }

    return s + c;
}







float FastTwoSum1(float a, float b, float* y)
{
    /* |a| >= |b| */
    float x = a + b;
    *y = (a - x) + b;
    return x;
}

float FastTwoSum2(float a, float b, float* y)
{
    /* |a| >= |b| */
    float x = a + b;
    *y = (x - a) - b;
    return x;
}

void FastTwoSum3(const double a, const double b, double* s, double* t)
{
    *s = a + b;
    *t = b - (*s - a);
    return;
}

void TwoSum3(const double a, const double b, double* s, double* t)
{
    *s = a + b;
    double z = *s - a;
    *t = (a - (*s - z)) + (b - z);
    return;
}

float TwoSum2(float a, float b, float* y)
{
    float x = a + b;
    float z = x - a;
    *y = (a - (x - z)) + (b - z);
    return x;
}

int bsf_flt_sum2_cmp(const void* pl, const void* pr)
{
    float l = fabsf(*(const float*)pl);
    float r = fabsf(*(const float*)pr);

    return l < r ? -1 : l > r ? 1 : 0;
}

float bsf_flt_sum2(const float* pdata, int length)
{
    size_t k = length * sizeof(float);
    float* m = (float*)memcpy(malloc(k), pdata, k);
    float* p = m;

    qsort(p, length, sizeof(float), bsf_flt_sum2_cmp);

    float s = *p++;

    while (length > 1)
    {
        s += *p++;
        --length;
    }

    free(m);
    return s;
}

/* https://en.wikipedia.org/wiki/Kahan_summation_algorithm */
float bsf_flt_sum_kahan0(const float* pdata, int length)
{
    float s = *pdata++;
    float c = 0;

    while (length > 1)
    {
        float y = *pdata++ - c;
        float t = s + y;
        c = (t - s) - y;
        s = t;
        --length;
    }

    return s;
}

float bsf_flt_sum_kahan22(const float* pdata, int length)
{
    float s = *pdata++;
    float c = 0;

    while (length > 1)
    {
        float y = *pdata++ - c;
        float t = s + y;
        c = (t - s) - y;
        s = t;
        --length;
    }

    return s;
}

float bsf_flt_sum_neumaierx(const float* pdata, int length)
{
    float s = 0;
    float c = 0;

    while (length > 0)
    {
        float v = *pdata++;
        float t = s + v;
        float z = t - s;
        c += (s - (t - z)) + (v - z);
        s = t;
        --length;
    }

    return s + c;
}

float bsf_flt_sum_klein(const float* pdata, int length)
{
    float s = 0;
    float cs = 0;
    float ccs = 0;

    while (length > 0)
    {
        float v = *pdata++;
        float t = s + v;
        float c = fabs(s) >= fabs(v) ? (s - t) + v : (v - t) + s;
        s = t;
        t = cs + c;
        float cc = fabs(cs) >= fabs(c) ? (cs - t) + c : (c - t) + cs;
        cs = t;
        ccs = ccs + cc;
        --length;
    }

    return s + cs + ccs;
}

float bsf_flt_sum_klein2(const float* pdata, int length)
{
    float s = 0;
    float cs = 0;
    float ccs = 0;
    float t;
    float c;
    float cc;
    float z;

    while (length > 0)
    {
        float v = *pdata++;
        TwoSum(s, v, t, c, z);
        s = t;
        TwoSum(cs, c, t, cc, z);
        cs = t;
        ccs = ccs + cc;
        --length;
    }

    return s + cs + ccs;
}

float bsf_flt_sum_klein3(const float* pdata, int length)
{
    float s = 0;
    float cs = 0;
    float ccs = 0;
    float cccs = 0;
    float t;
    float c;
    float cc;
    float ccc;
    float z;

    while (length > 0)
    {
        float v = *pdata++;
        TwoSum(s, v, t, c, z);
        s = t;
        TwoSum(cs, c, t, cc, z);
        cs = t;
        TwoSum(ccs, cc, t, ccc, z);
        ccs = t;
        cccs = cccs + ccc;
        --length;
    }

    return s + cs + ccs + cccs;
}

#define KLEINORDER  9

float bsf_flt_sum_kleinx(const float* pdata, int length)
{
    float s[KLEINORDER] = { 0 };
    float c[KLEINORDER] = { 0 };
    float t;
    float z;
    int i;

    s[0] = *pdata++;

    while (length > 1)
    {
        float v = *pdata++;

        TwoSum(s[0], v, t, c[0], z);
        s[0] = t;

        for (i = 1; i < KLEINORDER - 1; ++i)
        {
            TwoSum(s[i], c[i - 1], t, c[i], z);
            s[i] = t;
        }

        s[i] += c[i - 1];
        --length;
    }

    float sum = 0;

    for (i = 0; i < KLEINORDER; ++i)
    {
        printf(" c[%d]=%f\n", i, c[i]);
    }

    for (i = 0; i < KLEINORDER; ++i)
    {
        printf(" s[%d]=%f\n", i, s[i]);
        sum += s[i];
    }

    return sum;
}

float bsf_flt_sum_ogita_sum2s(const float* pdata, int length)
{
    float s = *pdata++;
    float c = 0;

    while (length > 1)
    {
        float y = *pdata++ - c;
        float t = s + y;
        c = (t - s) - y;
        s = t;
        --length;
    }

    return s;
}
